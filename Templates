from pyspark.sql import SparkSession
from pyspark.sql.functions import col, collect_list, struct, count, create_map

# Create SparkSession
spark = SparkSession.builder \
    .appName("AccNumCounts") \
    .getOrCreate()

# Assuming you have already loaded your dataset into a DataFrame called 'df'
# If not, you can load it using spark.read.csv() or any other suitable method

# Group by bank_name and routing_number
grouped_df = df.groupBy("bank_name", "routing_number")

# Create a struct of account_length and acc_num
struct_df = grouped_df.agg(collect_list(struct("account_length", "acc_num")).alias("acc_info_list"))

# Extract the account lengths and acc_nums from the list of structs
extracted_df = struct_df.select("bank_name", "routing_number", "acc_info_list.account_length", "acc_info_list.acc_num")

# Group by bank_name, routing_number, and account_length
grouped_counts_df = extracted_df.groupBy("bank_name", "routing_number", "account_length").agg(count("acc_num").alias("count"))

# Create a map of account_length and count
mapped_df = grouped_counts_df.groupBy("bank_name", "routing_number").agg(create_map("account_length", "count").alias("acc_length_counts"))

# Show the result
mapped_df.show(truncate=False)

# Stop SparkSession
spark.stop()
