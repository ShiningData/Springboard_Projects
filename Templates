# Collect variability data for normalization
daily_variability_data = daily_variability.collect()[0]
weekly_variability_data = weekly_variability.collect()[0]
bi_weekly_variability_data = bi_weekly_variability.collect()[0]
monthly_variability_data = monthly_variability.collect()[0]

# Normalizing variability data based on daily variability
variability_total_sum_balance = [
    1.0,
    weekly_variability_data['stddev_total_sum_balance'] / daily_variability_data['stddev_total_sum_balance'], 
    bi_weekly_variability_data['stddev_total_sum_balance'] / daily_variability_data['stddev_total_sum_balance'], 
    monthly_variability_data['stddev_total_sum_balance'] / daily_variability_data['stddev_total_sum_balance']
]

variability_avg_sum_balance = [
    1.0,
    weekly_variability_data['stddev_avg_sum_balance'] / daily_variability_data['stddev_avg_sum_balance'], 
    bi_weekly_variability_data['stddev_avg_sum_balance'] / daily_variability_data['stddev_avg_sum_balance'], 
    monthly_variability_data['stddev_avg_sum_balance'] / daily_variability_data['stddev_avg_sum_balance']
]

variability_total_avg_balance = [
    1.0,
    weekly_variability_data['stddev_total_avg_balance'] / daily_variability_data['stddev_total_avg_balance'], 
    bi_weekly_variability_data['stddev_total_avg_balance'] / daily_variability_data['stddev_total_avg_balance'], 
    monthly_variability_data['stddev_total_avg_balance'] / daily_variability_data['stddev_total_avg_balance']
]

variability_avg_avg_balance = [
    1.0,
    weekly_variability_data['stddev_avg_avg_balance'] / daily_variability_data['stddev_avg_avg_balance'], 
    bi_weekly_variability_data['stddev_avg_avg_balance'] / daily_variability_data['stddev_avg_avg_balance'], 
    monthly_variability_data['stddev_avg_avg_balance'] / daily_variability_data['stddev_avg_avg_balance']
]

# Plotting Variability
fig, axs = plt.subplots(2, 2, figsize=(15, 10))

variability_labels = ['Daily', 'Weekly', 'Bi-Weekly', 'Monthly']

axs[0, 0].bar(variability_labels, variability_total_sum_balance)
axs[0, 0].set_title('Normalized Std Dev of Total Sum Balance')
axs[0, 0].set_ylabel('Normalized Std Dev')

axs[0, 1].bar(variability_labels, variability_avg_sum_balance)
axs[0, 1].set_title('Normalized Std Dev of Avg Sum Balance')
axs[0, 1].set_ylabel('Normalized Std Dev')

axs[1, 0].bar(variability_labels, variability_total_avg_balance)
axs[1, 0].set_title('Normalized Std Dev of Total Avg Balance')
axs[1, 0].set_ylabel('Normalized Std Dev')

axs[1, 1].bar(variability_labels, variability_avg_avg_balance)
axs[1, 1].set_title('Normalized Std Dev of Avg Avg Balance')
axs[1, 1].set_ylabel('Normalized Std Dev')

# Adjust layout
plt.tight_layout()
plt.show()
=============================================================================================================================================================================
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_date, col, sum as _sum, avg, stddev, weekofyear, month, year, min, lit, udf
from pyspark.sql.types import IntegerType
import matplotlib.pyplot as plt

# Initialize Spark session
spark = SparkSession.builder.appName("DepositDataAggregation").getOrCreate()

# Define schema
schema = """
    rlt_n_pwr_id STRING,
    account_type STRING,
    src_load_date DATE,
    sum_balance DOUBLE,
    avg_balance DOUBLE,
    avg_int_rate DOUBLE
"""

# Sample data (top 20 rows)
data = [
    ("0003577133", "MMDA SWEEP", "2018-12-07", 1054543.02, 1054543.02, 0.5),
    ("0000782508", "MMDA", "2019-10-31", 272.72, 272.72, 1.0),
    ("0004764134", "MMDA", "2019-06-19", 763606.38, 763606.38, 0.75),
    ("0004675448", "MMDA", "2021-04-24", 39035.65, 39035.65, 0.5),
    ("0002467317", "MMDA SWEEP", "2010-08-01", 1582832.95, 1582832.95167, 0.179167),
    ("0002478749", "MMDA SWEEP", "2019-09-07", 2764943.09, 2764943.09, 4.75),
    ("0001885101", "IBDDA PF", "2024-01-10", 13536742.78, 565144.678333, 2.1125),
    ("0009769630", "IBDDA", "2023-09-01", 1022335.74, 46469.806364, 2.8),
    ("0005605238", "MMDA", "2023-05-24", 15140.24, 15140.24, 2.75),
    ("0000423802", "MMDA SWEEP", "2024-05-03", 3176147.99, 3176147.99, 4.35),
    ("0000771262", "MMDA", "2024-01-05", 476981.6, 476981.6, 2.75),
    ("0000425531", "MMDA", "2019-02-21", 52.49, 52.49, 2.75),
    ("0008467255", "IBDDA", "2024-05-10", 11386211.14, 1014323.933846, 2.834615),
    ("0008015505", "IBDDA", "2024-01-11", 5728917.21, 95482.209833, 2.75),
    ("0008113380", "Daily Interest Sweep", "2024-04-23", 7226167.24, 1033209.695714, 4.657142),
    ("0000296609", "MMDA SWEEP", "2023-12-07", 19452376.51, 19452376.51, 3.5),
    ("0000061532", "MMDA SWEEP", "2023-09-23", 3823496.18, 3823496.18, 3.15),
    ("0001802165", "MMDA SWEEP", "2024-03-24", 463278.18, 463278.18, 2.75),
    ("0004084439", "SUBACCOUNTING", "2024-04-02", 154748.73, 70337.215, 0.0),
    ("000891518", "IBDDA", "2024-02-24", 2337122.54, 1168561.27, 2.8)
]

# Create DataFrame
df = spark.createDataFrame(data, schema=schema)

# Add year column
df = df.withColumn("year", year(df.src_load_date))

# Calculate the minimum date
min_date = df.select(min(col("src_load_date"))).collect()[0][0]

# Define UDF to calculate bi-weekly period
def bi_weekly_period(start_date, current_date):
    diff_days = (current_date - start_date).days
    return (diff_days // 14) + 1

bi_weekly_udf = udf(bi_weekly_period, IntegerType())

# Add bi-weekly period column
df = df.withColumn("bi_weekly_period", bi_weekly_udf(lit(min_date), col("src_load_date")))

# Calculate unique combinations before aggregation
unique_combinations_before = df.select("rlt_n_pwr_id", "account_type").distinct().count()
print(f"Unique combinations before aggregation: {unique_combinations_before}")

# Aggregations
daily_agg = df.groupBy("rlt_n_pwr_id", "account_type", "year", to_date(col("src_load_date")).alias("date")).agg(
    _sum("sum_balance").alias("total_sum_balance"),
    avg("sum_balance").alias("avg_sum_balance"),
    _sum("avg_balance").alias("total_avg_balance"),
    avg("avg_balance").alias("avg_avg_balance")
)

weekly_agg = df.groupBy("rlt_n_pwr_id", "account_type", "year", weekofyear(col("src_load_date")).alias("week")).agg(
    _sum("sum_balance").alias("total_sum_balance"),
    avg("sum_balance").alias("avg_sum_balance"),
    _sum("avg_balance").alias("total_avg_balance"),
    avg("avg_balance").alias("avg_avg_balance")
)

bi_weekly_agg = df.groupBy("rlt_n_pwr_id", "account_type", "year", "bi_weekly_period").agg(
    _sum("sum_balance").alias("total_sum_balance"),
    avg("sum_balance").alias("avg_sum_balance"),
    _sum("avg_balance").alias("total_avg_balance"),
    avg("avg_balance").alias("avg_avg_balance")
)

monthly_agg = df.groupBy("rlt_n_pwr_id", "account_type", "year", month(col("src_load_date")).alias("month")).agg(
    _sum("sum_balance").alias("total_sum_balance"),
    avg("sum_balance").alias("avg_sum_balance"),
    _sum("avg_balance").alias("total_avg_balance"),
    avg("avg_balance").alias("avg_avg_balance")
)

# Filter out rows where all balance columns are 0.0
daily_agg = daily_agg.filter(
    (col("total_sum_balance") != 0.0) | 
    (col("avg_sum_balance") != 0.0) | 
    (col("total_avg_balance") != 0.0) | 
    (col("avg_avg_balance") != 0.0)
)

weekly_agg = weekly_agg.filter(
    (col("total_sum_balance") != 0.0) | 
    (col("avg_sum_balance") != 0.0) | 
    (col("total_avg_balance") != 0.0) | 
    (col("avg_avg_balance") != 0.0)
)

bi_weekly_agg = bi_weekly_agg.filter(
    (col("total_sum_balance") != 0.0) | 
    (col("avg_sum_balance") != 0.0) | 
    (col("total_avg_balance") != 0.0) | 
    (col("avg_avg_balance") != 0.0)
)

monthly_agg = monthly_agg.filter(
    (col("total_sum_balance") != 0.0) | 
    (col("avg_sum_balance") != 0.0) | 
    (col("total_avg_balance") != 0.0) | 
    (col("avg_avg_balance") != 0.0)
)

# Calculate unique combinations after aggregation
unique_combinations_after_daily = daily_agg.select("rlt_n_pwr_id", "account_type", "year").distinct().count()
unique_combinations_after_weekly = weekly_agg.select("rlt_n_pwr_id", "account_type", "year").distinct().count()
unique_combinations_after_bi_weekly = bi_weekly_agg.select("rlt_n_pwr_id", "account_type", "year").distinct().count()
unique_combinations_after_monthly = monthly_agg.select("rlt_n_pwr_id", "account_type", "year").distinct().count()

print(f"Unique combinations after daily aggregation: {unique_combinations_after_daily}")
print(f"Unique combinations after weekly aggregation: {unique_combinations_after_weekly}")
print(f"Unique combinations after bi-weekly aggregation: {unique_combinations_after_bi_weekly}")
print(f"Unique combinations after monthly aggregation: {unique_combinations_after_monthly}")

# Calculate variability (standard deviation) for each aggregation
daily_variability = daily_agg.agg(
    stddev("total_sum_balance").alias("stddev_total_sum_balance"),
    stddev("avg_sum_balance").alias("stddev_avg_sum_balance"),
    stddev("total_avg_balance").alias("stddev_total_avg_balance"),
    stddev("avg_avg_balance").alias("stddev_avg_avg_balance")
)

weekly_variability = weekly_agg.agg(
    stddev("total_sum_balance").alias("stddev_total_sum_balance"),
    stddev("avg_sum_balance").alias("stddev_avg_sum_balance"),
    stddev("total_avg_balance").alias("stddev_total_avg_balance"),
    stddev("avg_avg_balance").alias("stddev_avg_avg_balance")
)

bi_weekly_variability = bi_weekly_agg.agg(
    stddev("total_sum_balance").alias("stddev_total_sum_balance"),
    stddev("avg_sum_balance").alias("stddev_avg_sum_balance"),
    stddev("total_avg_balance").alias("stddev_total_avg_balance"),
    stddev("avg_avg_balance").alias("stddev_avg_avg_balance")
)

monthly_variability = monthly_agg.agg(
    stddev("total_sum_balance").alias("stddev_total_sum_balance"),
    stddev("avg_sum_balance").alias("stddev_avg_sum_balance"),
    stddev("total_avg_balance").alias("stddev_total_avg_balance"),
    stddev("avg_avg_balance").alias("stddev_avg_avg_balance")
)

# Collect data for plotting
daily_agg_data = daily_agg.select("date", "total_sum_balance", "total_avg_balance").collect()
weekly_agg_data = weekly_agg.select("week", "total_sum_balance", "total_avg_balance").collect()
bi_weekly_agg_data = bi_weekly_agg.select("bi_weekly_period", "total_sum_balance", "total_avg_balance").collect()
monthly_agg_data = monthly_agg.select("month", "total_sum_balance", "total_avg_balance").collect()

daily_variability_data = daily_variability.collect()[0]
weekly_variability_data = weekly_variability.collect()[0]
bi_weekly_variability_data = bi_weekly_variability.collect()[0]
monthly_variability_data = monthly_variability.collect()[0]

# Plotting Aggregations
fig, axs = plt.subplots(4, 1, figsize=(15, 20))

# Daily Aggregation
daily_dates = [row['date'] for row in daily_agg_data]
daily_total_sum_balance = [row['total_sum_balance'] for row in daily_agg_data]
daily_total_avg_balance = [row['total_avg_balance'] for row in daily_agg_data]

axs[0].plot(daily_dates, daily_total_sum_balance, label='Total Sum Balance')
axs[0].plot(daily_dates, daily_total_avg_balance, label='Total Avg Balance')
axs[0].set_title('Daily Aggregation')
axs[0].set_xlabel('Date')
axs[0].set_ylabel('Balance')
axs[0].legend()

# Weekly Aggregation
weekly_weeks = [row['week'] for row in weekly_agg_data]
weekly_total_sum_balance = [row['total_sum_balance'] for row in weekly_agg_data]
weekly_total_avg_balance = [row['total_avg_balance'] for row in weekly_agg_data]

axs[1].plot(weekly_weeks, weekly_total_sum_balance, label='Total Sum Balance')
axs[1].plot(weekly_weeks, weekly_total_avg_balance, label='Total Avg Balance')
axs[1].set_title('Weekly Aggregation')
axs[1].set_xlabel('Week')
axs[1].set_ylabel('Balance')
axs[1].legend()

# Bi-Weekly Aggregation
bi_weekly_periods = [row['bi_weekly_period'] for row in bi_weekly_agg_data]
bi_weekly_total_sum_balance = [row['total_sum_balance'] for row in bi_weekly_agg_data]
bi_weekly_total_avg_balance = [row['total_avg_balance'] for row in bi_weekly_agg_data]

axs[2].plot(bi_weekly_periods, bi_weekly_total_sum_balance, label='Total Sum Balance')
axs[2].plot(bi_weekly_periods, bi_weekly_total_avg_balance, label='Total Avg Balance')
axs[2].set_title('Bi-Weekly Aggregation')
axs[2].set_xlabel('Bi-Weekly Period')
axs[2].set_ylabel('Balance')
axs[2].legend()

# Monthly Aggregation
monthly_months = [row['month'] for row in monthly_agg_data]
monthly_total_sum_balance = [row['total_sum_balance'] for row in monthly_agg_data]
monthly_total_avg_balance = [row['total_avg_balance'] for row in monthly_agg_data]

axs[3].plot(monthly_months, monthly_total_sum_balance, label='Total Sum Balance')
axs[3].plot(monthly_months, monthly_total_avg_balance, label='Total Avg Balance')
axs[3].set_title('Monthly Aggregation')
axs[3].set_xlabel('Month')
axs[3].set_ylabel('Balance')
axs[3].legend()

# Adjust layout
plt.tight_layout()
plt.show()

# Plotting Variability
fig, axs = plt.subplots(2, 2, figsize=(15, 10))

variability_labels = ['Daily', 'Weekly', 'Bi-Weekly', 'Monthly']

variability_total_sum_balance = [
    daily_variability_data['stddev_total_sum_balance'], 
    weekly_variability_data['stddev_total_sum_balance'], 
    bi_weekly_variability_data['stddev_total_sum_balance'], 
    monthly_variability_data['stddev_total_sum_balance']
]

variability_avg_sum_balance = [
    daily_variability_data['stddev_avg_sum_balance'], 
    weekly_variability_data['stddev_avg_sum_balance'], 
    bi_weekly_variability_data['stddev_avg_sum_balance'], 
    monthly_variability_data['stddev_avg_sum_balance']
]

variability_total_avg_balance = [
    daily_variability_data['stddev_total_avg_balance'], 
    weekly_variability_data['stddev_total_avg_balance'], 
    bi_weekly_variability_data['stddev_total_avg_balance'], 
    monthly_variability_data['stddev_total_avg_balance']
]

variability_avg_avg_balance = [
    daily_variability_data['stddev_avg_avg_balance'], 
    weekly_variability_data['stddev_avg_avg_balance'], 
    bi_weekly_variability_data['stddev_avg_avg_balance'], 
    monthly_variability_data['stddev_avg_avg_balance']
]

axs[0, 0].bar(variability_labels, variability_total_sum_balance)
axs[0, 0].set_title('Std Dev of Total Sum Balance')
axs[0, 0].set_ylabel('Std Dev')

axs[0, 1].bar(variability_labels, variability_avg_sum_balance)
axs[0, 1].set_title('Std Dev of Avg Sum Balance')
axs[0, 1].set_ylabel('Std Dev')

axs[1, 0].bar(variability_labels, variability_total_avg_balance)
axs[1, 0].set_title('Std Dev of Total Avg Balance')
axs[1, 0].set_ylabel('Std Dev')

axs[1, 1].bar(variability_labels, variability_avg_avg_balance)
axs[1, 1].set_title('Std Dev of Avg Avg Balance')
axs[1, 1].set_ylabel('Std Dev')

Title: Sensitivity Analysis of Interest Change Impact on Deposit Balance

Introduction:
We conducted an analysis to determine the best aggregation level (daily, weekly, bi-weekly, and monthly) for evaluating the sensitivity of deposit balances to changes in interest rates. This analysis involved aggregating deposit data at different time intervals and evaluating data loss, variability, and the practical implications for sensitivity analysis.

Methodology:

Data Aggregation:

Aggregated data at daily, weekly, bi-weekly, and monthly levels.
Included the year column in groupings to account for temporal changes over multiple years.
Filtered out rows where all balance metrics were zero to ensure meaningful analysis.
Metrics Evaluated:

Total Sum Balance: Sum of deposit balances.
Average Sum Balance: Average of deposit balances.
Total Average Balance: Sum of average balances.
Average Average Balance: Average of average balances.
Standard Deviation (Variability): Calculated for each metric to assess consistency over time.
Comparison Criteria:

Data Retention: Number of unique rlt_n_pwr_id and account_type combinations retained after aggregation.
Variability: Standard deviation of balance metrics at each aggregation level.
Results:

Data Retention:

Daily Aggregation: Retained the highest number of unique combinations, providing the most granular detail but potentially high noise.
Weekly Aggregation: Balanced data retention and noise, with fewer unique combinations than daily but more than bi-weekly and monthly.
Bi-Weekly Aggregation: Further reduced unique combinations, potentially smoothing out some noise but also losing more detail.
Monthly Aggregation: Retained the least number of unique combinations, offering the most smoothed and aggregated view.
Variability Analysis:

Daily Aggregation: Highest variability due to day-to-day fluctuations, which can obscure underlying trends.
Weekly Aggregation: Reduced variability compared to daily, capturing short-term trends without as much noise.
Bi-Weekly Aggregation: Further reduced variability, balancing between capturing trends and smoothing out noise.
Monthly Aggregation: Lowest variability, capturing long-term trends but potentially missing short-term fluctuations and sensitivity.
Conclusion and Recommendation:
Based on the analysis, we recommend using weekly aggregation for the sensitivity analysis of interest change impact on deposit balances. Here's why:

Balance Between Detail and Noise: Weekly aggregation strikes a balance between retaining sufficient detail and reducing day-to-day noise. It allows us to capture short-term trends and changes without being overwhelmed by high-frequency variability.

Data Retention: Weekly aggregation retains a significant number of unique combinations of rlt_n_pwr_id and account_type, ensuring that our analysis remains representative of the overall dataset.

Manageable Variability: Weekly aggregation reduces the variability compared to daily aggregation, making it easier to identify meaningful trends and correlations with interest rate changes.

Practical Considerations: Weekly data aligns well with many financial reporting cycles and operational decision-making processes, making it a practical choice for ongoing analysis and reporting.

Next Steps:

Implement weekly aggregation for future sensitivity analyses.
Monitor results and adjust as needed based on new insights or changing business requirements.
Consider running periodic checks with other aggregation levels to validate the consistency of findings.

# Adjust layout
plt.tight_layout()
plt.show()
