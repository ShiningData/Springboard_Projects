from pyspark.sql import SparkSession
from pyspark.sql.functions import col, collect_list, struct, count

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Bank Account Analysis") \
    .getOrCreate()

# Sample data
data = [
    ("Bank A", "123456", 5, "111111"),
    ("Bank A", "123456", 5, "222222"),
    ("Bank A", "123456", 7, "333333"),
    ("Bank B", "789012", 5, "444444"),
    ("Bank B", "789012", 7, "555555"),
    ("Bank B", "789012", 7, "666666")
]

# Create DataFrame
df = spark.createDataFrame(data, ["bank_name", "routing_number", "account_length", "acc_num"])

# Group by bank_name and routing_number, and create a column with a dictionary of counts of acc_num for associated account_length
result_df = df.groupby("bank_name", "routing_number") \
    .agg(collect_list(struct("account_length", "acc_num")).alias("account_info")) \
    .select("bank_name", "routing_number", 
            {str(i[0]): count(i[1]).alias("count") for i in df.select("account_length").distinct().collect()}) \
    .withColumn("account_info", col("account_info").cast("string"))

# Show result
result_df.show(truncate=False)

# Stop SparkSession
spark.stop()
