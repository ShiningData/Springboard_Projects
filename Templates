# Setup PyCaret for training data
clf_setup = setup(train_data_model, target='DenialStatus', session_id=123, silent=True, use_gpu=True)

# Compare models and select the best one
best_model = compare_models()

# Tune the best model
tuned_model = tune_model(best_model)

# Finalize the model
final_model = finalize_model(tuned_model)

# Save the model
save_model(final_model, 'final_model')

print("Model training and saving completed.")

# Predict on the unseen data
predictions = predict_model(final_model, data=unseen_data_model)

# Add ClaimID and ServiceLineID back to the predictions
predictions[reference_cols] = unseen_data[reference_cols]

# Aggregate predictions to determine the final claim label
claim_predictions = predictions.groupby('ClaimID').agg({
    'Label': lambda x: 'Denied' if 'Denied' in x.values else 'Accepted'
}).reset_index()

# Merge the claim-level predictions with the original service line level data
final_predictions = pd.merge(unseen_data, claim_predictions, on='ClaimID', suffixes=('', '_Claim'))

# Save the train and unseen data with predictions
train_data.to_csv('train_data.csv', index=False)
unseen_data.to_csv('unseen_data.csv', index=False)
final_predictions.to_csv('predictions.csv', index=False)

print("Train, unseen data, and predictions saved.")
