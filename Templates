from pyspark.sql import SparkSession
from pyspark.sql.functions import col, collect_list, struct, count, create_map

# Create SparkSession
spark = SparkSession.builder \
    .appName("AccNumCounts") \
    .getOrCreate()

# Assuming you have already loaded your dataset into a DataFrame called 'df'
# If not, you can load it using spark.read.csv() or any other suitable method

# Group by bank_name and routing_number
grouped_df = df.groupBy("bank_name", "routing_number")

# Create a struct of account_length and acc_num
struct_df = grouped_df.agg(collect_list(struct("account_length", "acc_num")).alias("acc_info_list"))

# Count the occurrences of each (account_length, acc_num) combination and create a map
mapped_df = struct_df.withColumn("acc_num_counts",
                                 create_map("account_length", count("acc_num")).alias("acc_length_counts"))

# Select required columns
result_df = mapped_df.select("bank_name", "routing_number", "acc_length_counts")

# Show the result
result_df.show(truncate=False)

# Stop SparkSession
spark.stop()
