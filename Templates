from pyspark.sql import SparkSession
from pyspark.sql import Window
from pyspark.sql.functions import col, sum as sum_, round

# Initialize Spark Session
spark = SparkSession.builder.appName("Percentage Calculation without Collect").getOrCreate()

# Assuming 'df' is your PySpark DataFrame

# Define a window specification without partitioning (covers entire DataFrame)
windowSpec = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)

# Calculate the sum over the entire DataFrame for the column "2023-10-01"
sum_column = sum_("2023-10-01").over(windowSpec)

# Calculate the percentage and round to 2 decimal places
df = df.withColumn("percentage", round((col("2023-10-01") / sum_column) * 100, 2))

# Show the result (optional)
df.show()
