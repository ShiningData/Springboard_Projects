from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Define the window spec
windowSpec = Window.partitionBy('account_length')

# Use the window spec in an aggregation
df_new = df.withColumn('acc_num_count', F.count('acc_num').over(windowSpec))
