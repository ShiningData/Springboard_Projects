import pandas as pd
import catboost as cb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import optuna
import pickle

# Load your dataset (replace with your actual dataset)
# Assuming your dataset is in a pandas dataframe called df
df = pd.read_csv('your_data.csv')

# Features and target
X = df.drop(columns=['customerResultCode'])
y = df['customerResultCode']

# Split the data into training and test sets (stratified to handle imbalance)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Identify categorical feature indices
categorical_features_indices = [i for i, col in enumerate(X.columns) if X[col].dtype == 'object']

# Objective function for Optuna
def objective(trial):
    param = {
        'iterations': trial.suggest_int('iterations', 500, 1000),
        'depth': trial.suggest_int('depth', 4, 10),
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10),
        'border_count': trial.suggest_int('border_count', 32, 255),
        'loss_function': 'MultiClass',
        'eval_metric': 'MultiClass',
        'random_seed': 42,
        'auto_class_weights': 'Balanced',  # Handle imbalance
        'cat_features': categorical_features_indices
    }
    
    # Train the model with these parameters
    model = cb.CatBoostClassifier(**param)
    model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=0, early_stopping_rounds=100)
    
    # Get predictions and score
    preds = model.predict(X_test)
    score = classification_report(y_test, preds, output_dict=True)
    
    # Returning the macro average f1-score as the objective
    return score['macro avg']['f1-score']

# Hyperparameter optimization with Optuna
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=30)

# Best parameters from the study
best_params = study.best_params
print("Best parameters: ", best_params)

# Retrain the model using the best hyperparameters
best_model = cb.CatBoostClassifier(
    iterations=best_params['iterations'],
    depth=best_params['depth'],
    learning_rate=best_params['learning_rate'],
    l2_leaf_reg=best_params['l2_leaf_reg'],
    border_count=best_params['border_count'],
    loss_function='MultiClass',
    eval_metric='MultiClass',
    random_seed=42,
    auto_class_weights='Balanced',
    cat_features=categorical_features_indices
)

# Fit the best model
best_model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=100, early_stopping_rounds=100)

# Save the model as a pickle file
model_filename = 'catboost_multiclass_model_no_pool.pkl'
with open(model_filename, 'wb') as f:
    pickle.dump(best_model, f)

print(f"Model saved as {model_filename}")

# Final predictions and evaluation
final_preds = best_model.predict(X_test)
print(classification_report(y_test, final_preds))

# Load the model from the pickle file
with open(model_filename, 'rb') as f:
    loaded_model = pickle.load(f)

# You can now use the loaded_model to make predictions
loaded_preds = loaded_model.predict(X_test)
print("Classification Report for Loaded Model:")
print(classification_report(y_test, loaded_preds))


=====================

To evaluate model predictions without actual target values, you can follow these steps:

1. **Predict with the model**: Use the trained model to make predictions on the new dataset. Since there is no target variable in the new data, you will be focusing on prediction probabilities and predicted classes.

   ```python
   predictions = model.predict(new_data)
   probabilities = model.predict_proba(new_data)
   ```

2. **Analyze predicted probabilities**: Since you don’t have the actual target, you can analyze the confidence of the predictions by inspecting the prediction probabilities. Higher confidence levels in predictions (e.g., probabilities close to 1 for a specific class) can give some insight into the model’s certainty.

3. **Leverage Unsupervised Validation**:
   - **Cluster Analysis**: You can apply unsupervised clustering techniques (e.g., k-means) to see if your predicted classes form distinct clusters. This can give you a sense of whether the predictions group well, providing a sanity check on the predictions.
   - **Outlier Detection**: Apply techniques like isolation forests or autoencoders to detect any unusual or outlier cases in the new data. You can assess whether the model's predictions for these outliers differ significantly.

4. **Business or domain rules**: If there are any business rules, domain knowledge, or known patterns, you can compare the model’s predictions against these rules. For example, if certain features suggest that a specific class is more likely, you can check if the model aligns with these expectations.

5. **Confidence-based flagging**: If prediction probabilities are low or ambiguous (close to 0.5 for several classes), you can flag those instances for further investigation or manual review.

By focusing on the model's prediction certainty, clustering, and domain rules, you can still gather useful insights from the predictions even without having the actual target variable.
