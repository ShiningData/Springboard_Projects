from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as spark_sum, when, lit

# Start Spark session
spark = SparkSession.builder.appName("Dataframe Join").getOrCreate()

# Define schema and data for DataFrame 1 and 2
data1 = [
    ("rule1", "active", "2023-01-01", "business_rule_1"),
    ("rule2", "inactive", "2022-05-01", "business_rule_2"),
    ("rule3", "active", "2023-03-01", "business_rule_3"),
]
columns1 = ["office_type_rule_name", "status", "effective_date", "business_rule"]
df1 = spark.createDataFrame(data1, columns1)

data2 = [
    ("rule1", "01", 2021, 10),
    ("rule1", "01", 2022, 12),
    ("rule1", "01", 2023, 14),
    ("rule1", "01", 2024, 16),
    ("rule2", "01", 2021, 20),
    ("rule2", "01", 2022, 25),
    ("rule2", "01", 2023, 18),
    ("rule2", "01", 2024, 22),
]
columns2 = ["office_type_rule_name", "execution_month", "execution_year", "num_execution"]
df2 = spark.createDataFrame(data2, columns2)

# Step 1: Preprocess the second table - Group by 'office_type_rule_name' and 'execution_year'
current_year = 2025
current_month = 7

aggregated_df2 = df2.groupBy("office_type_rule_name", "execution_year").agg(
    spark_sum("num_execution").alias("yearly_num_execution")
)

# Step 2: Pivot the aggregated DataFrame to get each year as a separate column
pivot_df2 = aggregated_df2.groupBy("office_type_rule_name").pivot("execution_year").sum("yearly_num_execution")

# Step 3: Handle the additional column for the first half of the current year
half_year_col_name = "Jan {} - June {}".format(current_year, current_year)

# Create a new column for the first half of 2025 if we are in July 2025
pivot_df2 = pivot_df2.withColumn(
    half_year_col_name,
    when(lit(current_month >= 7), lit("Half Year Completed")).otherwise(lit(None))
)

# Step 4: Join the base table (df1) with the pivoted second table (pivot_df2)
final_df = df1.join(pivot_df2, on="office_type_rule_name", how="left")

# Show the final DataFrame
final_df.show(truncate=False)

# Stop Spark session
spark.stop()
