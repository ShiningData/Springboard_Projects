Approach 1: Using Apache POI with Spark

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
import pandas as pd
from io import BytesIO

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Excel Reader with POI") \
    .getOrCreate()

# Define a function to read Excel using Apache POI (via pandas)
def read_excel_with_poi(file_path):
    # Read Excel with pandas (which uses POI under the hood)
    pd_df = pd.read_excel(file_path)
    
    # Convert pandas DataFrame to Spark DataFrame
    spark_df = spark.createDataFrame(pd_df)
    return spark_df

# For distributed processing, read file from each executor
def process_excel_partition(iterator):
    # This runs on each executor
    import pandas as pd
    for file_path in iterator:
        yield pd.read_excel(file_path)

# If you know the schema in advance (recommended for large files)
schema = StructType([
    StructField("Column1", StringType(), True),
    StructField("Column2", IntegerType(), True),
    StructField("Column3", DoubleType(), True)
])

# For a single Excel file
excel_df = read_excel_with_poi("path/to/your/file.xlsx")

# For multiple Excel files (distributed processing)
file_list = spark.sparkContext.parallelize(["file1.xlsx", "file2.xlsx"])
excel_dfs = file_list.flatMap(process_excel_partition)
excel_df = spark.createDataFrame(excel_dfs, schema)

excel_df.show()


Approach 2: Converting Excel to CSV first, then reading with Spark

from pyspark.sql import SparkSession
import pandas as pd
import os

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Excel to CSV Reader") \
    .getOrCreate()

# Step 1: Convert Excel to CSV (can be done outside of Spark)
def convert_excel_to_csv(excel_path, csv_path):
    # Read Excel file
    excel_df = pd.read_excel(excel_path)
    
    # Write to CSV
    excel_df.to_csv(csv_path, index=False)
    
    return csv_path

# Convert the file
excel_file = "path/to/your/file.xlsx"
csv_file = "path/to/temp/converted_file.csv"
convert_excel_to_csv(excel_file, csv_file)

# Step 2: Read the CSV with Spark
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv(csv_file)

# Show the data
df.show()

# Optional: Clean up the temporary CSV file
# os.remove(csv_file)

# For large-scale processing, you could implement this as a Spark job
def process_excel_files(excel_files):
    for excel_file in excel_files:
        csv_file = excel_file.replace(".xlsx", ".csv")
        convert_excel_to_csv(excel_file, csv_file)
        yield csv_file

# Process multiple files and create a list of CSV files
excel_files = ["file1.xlsx", "file2.xlsx", "file3.xlsx"]
csv_files = list(process_excel_files(excel_files))

# Read all CSV files at once
combined_df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv(csv_files)

combined_df.show()
