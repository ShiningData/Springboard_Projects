from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr, map_concat, coalesce, collect_list, create_map

# Create SparkSession
spark = SparkSession.builder \
    .appName("Merge Datasets and Update Dictionary") \
    .getOrCreate()

# Sample data for demonstration
data1 = [("123456789", {"11": 40, "5": 15, "4": 12}),
         ("987654321", {"10": 20, "6": 10, "4": 8})]
data2 = [("123456789", {"11": 20, "3": 5}),
         ("987654321", {"5": 25, "4": 15})]

# Define schemas
schema1 = ["routing_number", "bank_info"]
schema2 = ["routing_number", "bank_info"]

# Create DataFrames
df1 = spark.createDataFrame(data1, schema=schema1)
df2 = spark.createDataFrame(data2, schema=schema2)

# Perform the merge and update
merged_df = df1.join(df2, "routing_number", "outer") \
    .select(
        col("routing_number"),
        expr("""
            aggregate(
                concat(
                    collect_list(coalesce(df1.bank_info, map())),
                    collect_list(coalesce(df2.bank_info, map()))
                ),
                map(),
                (acc, x) -> aggregate(
                    x,
                    acc,
                    (a, y) -> map_concat(a, y, (k, v1, v2) -> coalesce(v1, 0) + coalesce(v2, 0))
                )
            )
        """).alias("bank_info")
    )

# Show the result
merged_df.show(truncate=False)

# Stop SparkSession
spark.stop()
