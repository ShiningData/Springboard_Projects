# Build Ensemble Model with Top 3 Performing Models

# This code should be added to your main notebook after evaluating all individual models
# and before the final model evaluation section

print("\n-------------- Building Ensemble Model from Top 3 Models --------------")

# 1. Identify the top 3 performing models based on validation RMSE
sorted_models = sorted(results.items(), key=lambda x: x[1]['val_rmse'])
top_3_models = sorted_models[:3]
print(f"Top 3 performing models:")
for i, (model_name, metrics) in enumerate(top_3_models):
    print(f"{i+1}. {model_name}: Validation RMSE = {metrics['val_rmse']:.2f}, R² = {metrics['val_r2']:.4f}")

# 2. Create pipelines for each top model, preserving their hyperparameters
top_model_pipelines = []

for model_name, metrics in top_3_models:
    if model_name == 'Linear Regression':
        model = LinearRegression()
    elif model_name == 'Ridge Regression':
        # Use best parameters if available from grid search
        if best_model_name == 'Ridge Regression' and hasattr(grid_search, 'best_params_'):
            alpha = grid_search.best_params_.get('model__alpha', 1.0)
        else:
            alpha = 1.0
        model = Ridge(alpha=alpha)
    elif model_name == 'Lasso Regression':
        # Use best parameters if available from grid search
        if best_model_name == 'Lasso Regression' and hasattr(grid_search, 'best_params_'):
            alpha = grid_search.best_params_.get('model__alpha', 0.1)
        else:
            alpha = 0.1
        model = Lasso(alpha=alpha)
    elif model_name == 'ElasticNet':
        # Use best parameters if available from grid search
        if best_model_name == 'ElasticNet' and hasattr(grid_search, 'best_params_'):
            alpha = grid_search.best_params_.get('model__alpha', 0.1)
            l1_ratio = grid_search.best_params_.get('model__l1_ratio', 0.5)
        else:
            alpha = 0.1
            l1_ratio = 0.5
        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)
    elif model_name == 'Random Forest':
        # Use best parameters if available from grid search
        if best_model_name == 'Random Forest' and hasattr(grid_search, 'best_params_'):
            n_estimators = grid_search.best_params_.get('model__n_estimators', 100)
            max_depth = grid_search.best_params_.get('model__max_depth', None)
            min_samples_split = grid_search.best_params_.get('model__min_samples_split', 2)
        else:
            n_estimators = 100
            max_depth = None
            min_samples_split = 2
        model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, 
                                     min_samples_split=min_samples_split, random_state=42)
    elif model_name == 'Gradient Boosting':
        # Use best parameters if available from grid search
        if best_model_name == 'Gradient Boosting' and hasattr(grid_search, 'best_params_'):
            n_estimators = grid_search.best_params_.get('model__n_estimators', 100)
            learning_rate = grid_search.best_params_.get('model__learning_rate', 0.1)
            max_depth = grid_search.best_params_.get('model__max_depth', 3)
        else:
            n_estimators = 100
            learning_rate = 0.1
            max_depth = 3
        model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate,
                                         max_depth=max_depth, random_state=42)
    
    # Create a complete pipeline for each model
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('selector', SelectKBest(f_regression, k=15)),
        ('model', model)
    ])
    
    # Add to our list of top models with name for reference
    top_model_pipelines.append((model_name, pipeline))
    
# 3. Implement a Voting Ensemble (average predictions)
from sklearn.ensemble import VotingRegressor

# Create the voting regressor from our top models
voting_ensemble = VotingRegressor(
    estimators=top_model_pipelines,
    weights=None  # Equal weights to start
)

print("\n--- Training Voting Ensemble (Equal Weights) ---")
# Train the voting ensemble on combined training and validation data
voting_ensemble.fit(X_train_full, y_train_full)

# Evaluate on the test set
y_voting_pred = voting_ensemble.predict(X_test)
voting_rmse = np.sqrt(mean_squared_error(y_test, y_voting_pred))
voting_r2 = r2_score(y_test, y_voting_pred)
voting_mae = mean_absolute_error(y_test, y_voting_pred)

print(f"Voting Ensemble Performance:")
print(f"RMSE: {voting_rmse:.2f}")
print(f"MAE: {voting_mae:.2f}")
print(f"R²: {voting_r2:.4f}")

# 4. Implement a Weighted Voting Ensemble based on validation performance
# Calculate weights inversely proportional to validation RMSE
val_rmses = [metrics['val_rmse'] for _, metrics in top_3_models]
weights = [1/(rmse**2) for rmse in val_rmses]  # Square to emphasize differences
weights = [w/sum(weights) for w in weights]  # Normalize to sum to 1

# Create weighted ensemble
weighted_ensemble = VotingRegressor(
    estimators=top_model_pipelines,
    weights=weights
)

print("\n--- Training Weighted Voting Ensemble ---")
print(f"Weights based on validation performance:")
for (model_name, _), weight in zip([m for m in top_3_models], weights):
    print(f"{model_name}: {weight:.4f}")

# Train the weighted ensemble
weighted_ensemble.fit(X_train_full, y_train_full)

# Evaluate on the test set
y_weighted_pred = weighted_ensemble.predict(X_test)
weighted_rmse = np.sqrt(mean_squared_error(y_test, y_weighted_pred))
weighted_r2 = r2_score(y_test, y_weighted_pred)
weighted_mae = mean_absolute_error(y_test, y_weighted_pred)

print(f"\nWeighted Ensemble Performance:")
print(f"RMSE: {weighted_rmse:.2f}")
print(f"MAE: {weighted_mae:.2f}")
print(f"R²: {weighted_r2:.4f}")

# 5. Stack the models with a meta-learner
from sklearn.ensemble import StackingRegressor
import numpy as np

print("\n--- Training Stacking Ensemble ---")

# Define base models from our top 3
base_models = top_model_pipelines

# Use Ridge as the meta-learner (final estimator)
meta_learner = Ridge(alpha=1.0)

# Create the stacking ensemble
stacking_ensemble = StackingRegressor(
    estimators=base_models,
    final_estimator=meta_learner,
    cv=5  # 5-fold cross-validation for base models
)

# Train the stacking ensemble
stacking_ensemble.fit(X_train_full, y_train_full)

# Evaluate on the test set
y_stacking_pred = stacking_ensemble.predict(X_test)
stacking_rmse = np.sqrt(mean_squared_error(y_test, y_stacking_pred))
stacking_r2 = r2_score(y_test, y_stacking_pred)
stacking_mae = mean_absolute_error(y_test, y_stacking_pred)

print(f"Stacking Ensemble Performance:")
print(f"RMSE: {stacking_rmse:.2f}")
print(f"MAE: {stacking_mae:.2f}")
print(f"R²: {stacking_r2:.4f}")

# 6. Compare all three ensemble methods
print("\n--- Ensemble Methods Comparison ---")
ensemble_models = {
    'Voting Ensemble (Equal Weights)': {'rmse': voting_rmse, 'mae': voting_mae, 'r2': voting_r2},
    'Voting Ensemble (Weighted)': {'rmse': weighted_rmse, 'mae': weighted_mae, 'r2': weighted_r2},
    'Stacking Ensemble': {'rmse': stacking_rmse, 'mae': stacking_mae, 'r2': stacking_r2}
}

# Print comparison table
print(f"{'Model':<30} {'RMSE':>12} {'MAE':>12} {'R²':>10}")
print("-" * 66)
for name, metrics in ensemble_models.items():
    print(f"{name:<30} {metrics['rmse']:>12.2f} {metrics['mae']:>12.2f} {metrics['r2']:>10.4f}")

# Find the best ensemble method
best_ensemble_name = min(ensemble_models.items(), key=lambda x: x[1]['rmse'])[0]
best_ensemble_rmse = ensemble_models[best_ensemble_name]['rmse']
print(f"\nBest ensemble method: {best_ensemble_name} with RMSE: {best_ensemble_rmse:.2f}")

# 7. Compare the best ensemble with the best single model
print("\n--- Best Single Model vs. Best Ensemble Model ---")
print(f"{'Model':<30} {'RMSE':>12} {'MAE':>12} {'R²':>10}")
print("-" * 66)
print(f"Best Single Model ({best_model_name}){' '*(22-len(best_model_name))} {test_rmse:>12.2f} {test_mae:>12.2f} {test_r2:>10.4f}")
print(f"{best_ensemble_name}{' '*(30-len(best_ensemble_name))} {ensemble_models[best_ensemble_name]['rmse']:>12.2f} {ensemble_models[best_ensemble_name]['mae']:>12.2f} {ensemble_models[best_ensemble_name]['r2']:>10.4f}")

# Calculate improvement
improvement = (test_rmse - ensemble_models[best_ensemble_name]['rmse']) / test_rmse * 100
print(f"\nImprovement from best single model to best ensemble: {improvement:.2f}%")

# 8. Select the best overall model (single or ensemble)
if ensemble_models[best_ensemble_name]['rmse'] < test_rmse:
    print("\nThe best ensemble model outperforms the best single model!")
    if best_ensemble_name == 'Voting Ensemble (Equal Weights)':
        final_ensemble_model = voting_ensemble
    elif best_ensemble_name == 'Voting Ensemble (Weighted)':
        final_ensemble_model = weighted_ensemble
    else:  # Stacking Ensemble
        final_ensemble_model = stacking_ensemble
    
    # Save the best ensemble model
    ensemble_filename = 'houston_housing_best_ensemble_model.pkl'
    with open(ensemble_filename, 'wb') as file:
        pickle.dump(final_ensemble_model, file)
    print(f"Best ensemble model saved to {ensemble_filename}")
    
    # For future predictions, use this model
    best_prediction_model = final_ensemble_model
else:
    print("\nThe best single model outperforms all ensemble methods!")
    best_prediction_model = final_model
    print(f"Continuing with the best single model: {best_model_name}")

# 9. Visualize model comparison
plt.figure(figsize=(12, 6))
model_names = list(ensemble_models.keys()) + [f"Best Single Model ({best_model_name})"]
rmse_values = [metrics['rmse'] for metrics in ensemble_models.values()] + [test_rmse]
r2_values = [metrics['r2'] for metrics in ensemble_models.values()] + [test_r2]

plt.subplot(1, 2, 1)
bars = plt.bar(model_names, rmse_values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
plt.xticks(rotation=45, ha='right')
plt.ylabel('RMSE (lower is better)')
plt.title('RMSE Comparison')

# Highlight the best model
best_idx = np.argmin(rmse_values)
bars[best_idx].set_color('darkgreen')

plt.subplot(1, 2, 2)
bars = plt.bar(model_names, r2_values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
plt.xticks(rotation=45, ha='right')
plt.ylabel('R² (higher is better)')
plt.title('R² Comparison')

# Highlight the best model
best_idx = np.argmax(r2_values)
bars[best_idx].set_color('darkgreen')

plt.tight_layout()
plt.show()
