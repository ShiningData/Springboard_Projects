from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr, map_concat, coalesce, map_concat_agg

# Create SparkSession
spark = SparkSession.builder \
    .appName("Merge Datasets and Update Dictionary") \
    .getOrCreate()

# Sample data for demonstration
data1 = [("123456789", {"11": 40, "5": 15, "4": 12}),
         ("987654321", {"10": 20, "6": 10, "4": 8})]
data2 = [("123456789", {"11": 20, "3": 5}),
         ("987654321", {"5": 25, "4": 15})]

# Define schemas
schema1 = ["routing_number", "bank_info"]
schema2 = ["routing_number", "bank_info"]

# Create DataFrames
df1 = spark.createDataFrame(data1, schema=schema1)
df2 = spark.createDataFrame(data2, schema=schema2)

# Perform the merge and update
merged_df = df1.join(df2, "routing_number", "outer") \
    .select(
        col("routing_number"),
        expr("map_concat(coalesce(df1.bank_info, map()), coalesce(df2.bank_info, map()))").alias("bank_info")
    ) \
    .groupBy("routing_number") \
    .agg(
        expr("map_concat_agg(bank_info)").alias("bank_info")
    )

# Show the result
merged_df.show(truncate=False)

# Stop SparkSession
spark.stop()
