from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, max as spark_max, concat_ws, to_date

# Initialize Spark session
spark = SparkSession.builder.appName("IncrementalAppend").getOrCreate()

# Define paths or table names
target_table_path = "/path/to/target_table"
source_table_path = "/path/to/source_table"

# Load the existing target table
target_table = spark.read.format("delta").load(target_table_path)

# Find the latest execution date in the target table
latest_date = target_table.select(spark_max("execution_date").alias("max_date")).collect()[0]["max_date"]

# Load the source table
source_table = spark.read.format("delta").load(source_table_path)

# Create execution_date from EXECUTION_MONTH and EXECUTION_YEAR
source_table = source_table.withColumn(
    "execution_date",
    to_date(concat_ws("/", col("EXECUTION_MONTH"), lit(1), col("EXECUTION_YEAR")), "MM/dd/yyyy")
)

# Filter new data based on the latest execution date
new_data = source_table.filter(col("execution_date") > lit(latest_date))

# Append new data to the target table
updated_table = target_table.union(new_data)

# Write the updated table back
updated_table.write.format("delta").mode("overwrite").save(target_table_path)
