from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Initialize Spark Session
spark = SparkSession.builder.appName("aggregate_accounts").getOrCreate()

# Assuming 'df' is your original DataFrame

# Define the window spec
windowSpec = Window.partitionBy('bank_name', 'routing_number', 'account_length')

# Count the occurrences of each account_length within each window partition
df_with_count = df.withColumn('acc_num_count', F.count('acc_num').over(windowSpec)) \
                  .distinct()

# Group by bank_name and routing_number, and collect lists of account_length and acc_num_count
grouped_df = df_with_count.groupBy('bank_name', 'routing_number') \
    .agg(
        F.collect_list('account_length').alias('account_lengths'),
        F.collect_list('acc_num_count').alias('acc_num_counts')
    )

# Create the map of account_length to acc_num_count for each bank_name and routing_number
df_new = grouped_df.withColumn('acc_info', F.map_from_arrays('account_lengths', 'acc_num_counts')) \
                   .drop('account_lengths', 'acc_num_counts')

# Show the result
df_new.show(truncate=False)

# Stop the Spark session
spark.stop()
