from pyspark.sql import functions as F

# Step 1: Dynamically identify the columns between "business_rule" and "effective_age"
# Get all columns
all_columns = spark_df.columns

# Identify the start and end indices for the dynamic columns
start_index = all_columns.index("business_rule") + 1
end_index = all_columns.index("effective_age")

# Dynamically get the column names between "business_rule" and "effective_age"
dynamic_columns = all_columns[start_index:end_index]

# Step 2: Create a new column based on the condition
spark_df = spark_df.withColumn(
    "New_Column",
    F.when(
        F.reduce(
            lambda acc, col: acc & (F.col(col) == "Not Used"),
            F.lit(True),
            *[F.col(col) for col in dynamic_columns]
        ),
        "Not Used"
    ).otherwise("Used")
)

# Step 3: Show the updated DataFrame
spark_df.show()
