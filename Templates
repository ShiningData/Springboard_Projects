from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark Session
spark = SparkSession.builder.appName("Unique ID Filter").getOrCreate()

# Assuming df1, df2, df3 are the three DataFrames with the column 'cust_pwr_id'
# and df_to_filter is the DataFrame you want to filter

# Step 1: Get unique 'cust_pwr_id' from each DataFrame
unique_ids_df1 = df1.select("cust_pwr_id").distinct()
unique_ids_df2 = df2.select("cust_pwr_id").distinct()
unique_ids_df3 = df3.select("cust_pwr_id").distinct()

# Step 2: Combine these unique values into a single DataFrame
combined_unique_ids = unique_ids_df1.union(unique_ids_df2).union(unique_ids_df3).distinct()

# Step 3: Filter the df_to_filter DataFrame using the combined_unique_ids DataFrame
filtered_df = df_to_filter.join(combined_unique_ids, df_to_filter["cust_pwr_id"] == combined_unique_ids["cust_pwr_id"], "inner")

# Show the result
filtered_df.show()
