# 6.2 Performance Metrics

These are the metrics that the model owner will/does use to determine if model change, redevelopment, or compensating measures are needed. These metrics will also drive overall performance rating of the model.

For each model business objective, describe the primary metric or set of metrics used to compare predicted outcomes to actual outcomes and measure the model performance. Discuss the rationale for the choice of each of these particular metrics.

## 6.2.1 Primary Metrics

For the healthcare claim denial predictor model, it is crucial to predict denial outcomes with high performance. A significant challenge when developing prediction models with healthcare claims data is balancing precision and recall, which is why we use the F1 score as our primary evaluation metric.

To clarify this evaluation metric, we need to detail precision and recall metrics in our binary classification model:

**Precision:**
Precision measures the accuracy of positive predictions made by the model. In the context of our claim denial predictor, precision is the ratio of correctly predicted denials to the total claims predicted as denials. A higher precision indicates that the model is making fewer false positive predictions, which means fewer claims are unnecessarily flagged for review.

Formula for Precision:
Precision = True Denials / (True Denials + False Denials)

**Recall:**
Recall measures the ability of the model to capture all actual denial instances. In our healthcare claims context, recall is the ratio of correctly predicted denials to the total actual denials. A higher recall indicates that the model is effectively identifying most of the claims that would be denied, which helps prevent revenue loss.

Formula for Recall:
Recall = True Denials / (True Denials + Missed Denials)

In healthcare claims data, there is typically an imbalance between denied and paid claims, with denials representing a smaller percentage of overall claims. This imbalance makes it challenging for the model to perform well on the minority class (denials). Accuracy alone can be misleading in such cases because it may appear high due to the dominant class (paid claims), while the model might struggle to identify actual denials.

F1 score is a harmonic mean of precision and recall, and it accounts for both false positives and false negatives. It is well-suited for our imbalanced claims dataset because it balances precision and recall, providing a more comprehensive evaluation of the model's performance.

Formula for F1 score:
F1 score = 2 * (Precision * Recall) / (Precision + Recall)

By using the F1 score, we consider the trade-off between precision and recall, making it a more suitable metric to assess the performance of our claim denial predictor on an imbalanced dataset. A high F1 score indicates that the model is both precise in its denial predictions and able to capture most of the actual denials, which is critical for optimizing revenue cycle operations.

## 6.2.2 Secondary Metrics

**Precision-Recall Curve:**
We also use the Precision-Recall Curve as one of our secondary metrics. This curve shows the trade-off between Precision and Recall across different probability thresholds. For various threshold values between 0 and 1, the precision and recall are calculated, and these pairs are plotted against each other. This allows us to identify the optimal threshold for our model that balances the need to catch actual denials while minimizing unnecessary claim reviews.

**ROC AUC Curve:**
To demonstrate that our model performs better than random selection, we use the Receiver Operating Characteristic (ROC) Curve. This is done by plotting the true positive rate against the false positive rate of the model. We also calculate the area under the curve (ROC AUC). A larger ROC AUC indicates better performance than a baseline random classifier (which has an ROC AUC of 0.5). With our ensemble approach combining AUTOGLUON and Catboost models, we've achieved an ROC AUC close to 0.9, confirming that our model significantly outperforms random classification.

**Cost Savings Metric:**
As an additional secondary metric, we track the estimated cost savings from prevented denials. By calculating the average cost of denial processing and the average revenue impact of denied claims, we can quantify the financial benefit of the model's implementation. This business-oriented metric helps demonstrate the ROI of the model to stakeholders.

**Claims Processing Efficiency:**
We also monitor the impact on claims processing time to ensure that the integration of our model into the workflow doesn't negatively affect overall processing efficiency while providing valuable denial prediction capabilities.​​​​​​​​​​​​​​​​
