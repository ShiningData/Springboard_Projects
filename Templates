from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, struct, collect_list, create_map

# Create SparkSession
spark = SparkSession.builder \
    .appName("AccNumCounts") \
    .getOrCreate()

# Assuming you have already loaded your dataset into a DataFrame called 'df'
# If not, you can load it using spark.read.csv() or any other suitable method

# Group by bank_name, routing_number, and account_length
grouped_df = df.groupBy("bank_name", "routing_number", "account_length")

# Aggregate by collecting a list of acc_num for each group
agg_df = grouped_df.agg(collect_list("acc_num").alias("acc_num_list"))

# Count the occurrences of acc_num for each account_length
count_df = agg_df.withColumn("acc_length_counts", create_map("account_length", count("acc_num_list")))

# Select required columns
result_df = count_df.select("bank_name", "routing_number", "acc_length_counts")

# Show the result
result_df.show(truncate=False)

# Stop SparkSession
spark.stop()
