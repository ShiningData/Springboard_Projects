import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from scipy.cluster.hierarchy import linkage, fcluster
from collections import defaultdict

# Sample data preparation (replace this with actual dataset)
# Assuming df is the dataframe created in the previous step

# Example: Creating a smaller sample dataset for illustration
data = {
    'RLTN_PWR_ID': np.random.randint(1, 10001, size=50000),
    'TIME_KEY': np.tile(pd.date_range(start='2023-01-01', periods=12, freq='MS').strftime('%Y-%m'), 50000 // 12),
    'TRAN_CD': np.random.choice([f'TRAN_{i}' for i in range(1, 62)], size=50000),
    'VOLUME': np.random.randint(0, 100, size=50000)
}

df = pd.DataFrame(data)

# Generate the pivot table as before
unique_rlt_ids = df['RLTN_PWR_ID'].unique()
unique_tran_cd = df['TRAN_CD'].unique()
time_keys = pd.date_range(start='2023-01-01', periods=12, freq='MS').strftime('%Y-%m').tolist()

all_combinations = pd.MultiIndex.from_product([unique_rlt_ids, time_keys, unique_tran_cd], names=['RLTN_PWR_ID', 'TIME_KEY', 'TRAN_CD'])
df_full = pd.DataFrame(index=all_combinations).reset_index()

df_merged = pd.merge(df_full, df, on=['RLTN_PWR_ID', 'TIME_KEY', 'TRAN_CD'], how='left')
df_merged['VOLUME'] = df_merged['VOLUME'].fillna(0)

pivot_table = df_merged.pivot_table(index='RLTN_PWR_ID', columns='TRAN_CD', values='VOLUME', aggfunc=lambda x: list(x))
for tran_cd in unique_tran_cd:
    if tran_cd not in pivot_table.columns:
        pivot_table[tran_cd] = [[0]*12] * len(pivot_table)

pivot_table = pivot_table[unique_tran_cd]
final_table = pivot_table.applymap(lambda x: x if isinstance(x, list) else [0]*12)

# Flatten the lists into a 2D array for each TRAN_CD
time_series_matrix = np.array([np.concatenate(final_table[col].values) for col in final_table.columns])

# Compute the cosine similarity matrix
cosine_sim = cosine_similarity(time_series_matrix)

# Transform cosine similarity to distance: distance = 1 - cosine_similarity
cosine_dist = 1 - cosine_sim

# Use hierarchical clustering to group similar TRAN_CDs
Z = linkage(cosine_dist, 'average')
max_d = 0.1  # Threshold for clustering (this might need adjustment based on your data)
clusters = fcluster(Z, max_d, criterion='distance')

# Create a dictionary to hold consolidation groups
consolidation_groups = defaultdict(list)
for i, cluster in enumerate(clusters):
    consolidation_groups[cluster].append(unique_tran_cd[i])

# Calculate the average cosine similarity for each group and count co-occurrences
consolidation_similarities = {}
co_occurrence_counts = {}

# Create a co-occurrence matrix
co_occurrence_matrix = pd.crosstab(df['RLTN_PWR_ID'], df['TRAN_CD'])

for cluster, tran_cds in consolidation_groups.items():
    if len(tran_cds) > 1:
        group_similarities = cosine_sim[np.ix_([unique_tran_cd.tolist().index(cd) for cd in tran_cds], 
                                               [unique_tran_cd.tolist().index(cd) for cd in tran_cds])]
        avg_similarity = np.mean(group_similarities[np.triu_indices_from(group_similarities, k=1)])
        consolidation_similarities[cluster] = (tran_cds, avg_similarity)
        
        # Calculate co-occurrence count
        co_occurrence_count = 0
        for rlt_id in co_occurrence_matrix.index:
            co_occurrence_count += all(co_occurrence_matrix.loc[rlt_id, tran_cd] > 0 for tran_cd in tran_cds)
        co_occurrence_counts[cluster] = co_occurrence_count

# Output the recommended consolidation groups with co-occurrence counts
for cluster, (tran_cds, avg_similarity) in consolidation_similarities.items():
    print(f"Group {cluster}: TRAN_CDs {tran_cds} with average cosine similarity {avg_similarity:.2f} and co-occurrence count {co_occurrence_counts[cluster]}")
