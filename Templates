# Import required libraries
from catboost import CatBoostClassifier
from autogluon.tabular import TabularPredictor
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have trained CatBoost and AutoGluon models and loaded the dataset
# catboost_model: your trained CatBoost model
# autogluon_model: your trained AutoGluon model

# Sample data
X_train = pd.read_csv('your_train_data.csv')  # Features used to train
y_train = pd.read_csv('your_target_data.csv')  # Target used to train

# CATBOOST FEATURE IMPORTANCE
def catboost_feature_importance(catboost_model, X_train):
    # Get feature importance from CatBoost
    catboost_importance = catboost_model.get_feature_importance(prettified=True)
    catboost_importance_df = pd.DataFrame(catboost_importance)
    
    # Plotting
    plt.figure(figsize=(10, 6))
    plt.barh(catboost_importance_df['Feature Id'], catboost_importance_df['Importances'], color='blue')
    plt.title("CatBoost Feature Importance")
    plt.xlabel("Importance")
    plt.ylabel("Feature")
    plt.show()

    return catboost_importance_df

# AUTOGLUON FEATURE IMPORTANCE
def autogluon_feature_importance(autogluon_model, X_train, y_train):
    # Use the autogluon predictor to get feature importance
    predictor = TabularPredictor(label='your_target_column').fit(X_train, y_train)
    autogluon_importance = predictor.feature_importance(X_train)

    # Plotting
    autogluon_importance_df = pd.DataFrame(autogluon_importance).reset_index()
    autogluon_importance_df.columns = ['Feature', 'Importance']
    
    plt.figure(figsize=(10, 6))
    plt.barh(autogluon_importance_df['Feature'], autogluon_importance_df['Importance'], color='green')
    plt.title("AutoGluon Feature Importance")
    plt.xlabel("Importance")
    plt.ylabel("Feature")
    plt.show()

    return autogluon_importance_df

# ENSEMBLE FEATURE IMPORTANCE (Average feature importance)
def ensemble_feature_importance(catboost_importance_df, autogluon_importance_df):
    # Merge importance dataframes on the feature name
    merged_importance = pd.merge(catboost_importance_df, autogluon_importance_df, left_on='Feature Id', right_on='Feature', how='inner')
    
    # Average the importance from both models
    merged_importance['Avg_Importance'] = merged_importance[['Importances', 'Importance']].mean(axis=1)
    
    # Plotting
    merged_importance.sort_values(by='Avg_Importance', ascending=False, inplace=True)
    
    plt.figure(figsize=(10, 6))
    plt.barh(merged_importance['Feature Id'], merged_importance['Avg_Importance'], color='purple')
    plt.title("Ensemble Feature Importance (Average)")
    plt.xlabel("Average Importance")
    plt.ylabel("Feature")
    plt.show()

    return merged_importance[['Feature Id', 'Avg_Importance']]

# Assuming you already have trained models
catboost_model = CatBoostClassifier().load_model('catboost_model.cbm')  # Load your trained CatBoost model
autogluon_model = TabularPredictor.load('autogluon_model')  # Load your trained AutoGluon model

# Get feature importance for CatBoost and AutoGluon
catboost_importance_df = catboost_feature_importance(catboost_model, X_train)
autogluon_importance_df = autogluon_feature_importance(autogluon_model, X_train, y_train)

# Calculate ensemble feature importance
ensemble_importance_df = ensemble_feature_importance(catboost_importance_df, autogluon_importance_df)

# Display the results
print(ensemble_importance_df)
