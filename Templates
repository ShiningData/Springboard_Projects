import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from scipy.cluster.hierarchy import linkage, fcluster
import fastdtw
from scipy.spatial.distance import euclidean
from collections import defaultdict

# Sample data preparation (replace this with actual dataset)
# Assuming df is the dataframe created in the previous step

# Example: Creating a smaller sample dataset for illustration
data = {
    'RLTN_PWR_ID': np.random.randint(1, 10001, size=50000),
    'TIME_KEY': np.tile(pd.date_range(start='2023-01-01', periods=12, freq='MS').strftime('%Y-%m'), 50000 // 12),
    'TRAN_CD': np.random.choice([f'TRAN_{i}' for i in range(1, 62)], size=50000),
    'VOLUME': np.random.randint(0, 100, size=50000)
}

df = pd.DataFrame(data)

# Generate the pivot table as before
unique_rlt_ids = df['RLTN_PWR_ID'].unique()
unique_tran_cd = df['TRAN_CD'].unique()
time_keys = pd.date_range(start='2023-01-01', periods=12, freq='MS').strftime('%Y-%m').tolist()

all_combinations = pd.MultiIndex.from_product([unique_rlt_ids, time_keys, unique_tran_cd], names=['RLTN_PWR_ID', 'TIME_KEY', 'TRAN_CD'])
df_full = pd.DataFrame(index=all_combinations).reset_index()

df_merged = pd.merge(df_full, df, on=['RLTN_PWR_ID', 'TIME_KEY', 'TRAN_CD'], how='left')
df_merged['VOLUME'] = df_merged['VOLUME'].fillna(0)

pivot_table = df_merged.pivot_table(index='RLTN_PWR_ID', columns='TRAN_CD', values='VOLUME', aggfunc=lambda x: list(x))
for tran_cd in unique_tran_cd:
    if tran_cd not in pivot_table.columns:
        pivot_table[tran_cd] = [[0]*12] * len(pivot_table)

pivot_table = pivot_table[unique_tran_cd]
final_table = pivot_table.applymap(lambda x: x if isinstance(x, list) else [0]*12)

# Compute the cosine similarity matrix
cosine_sim_matrix = pd.DataFrame(
    cosine_similarity(
        np.stack(final_table.apply(lambda x: np.array(x.tolist()), axis=1).values)
    ),
    index=unique_tran_cd,
    columns=unique_tran_cd
)

# Use hierarchical clustering to group similar TRAN_CDs
Z = linkage(1 - cosine_sim_matrix, 'average')
max_d = 0.1  # Threshold for clustering
clusters = fcluster(Z, max_d, criterion='distance')

# Create a dictionary to hold consolidation groups
consolidation_groups = defaultdict(list)
for i, cluster in enumerate(clusters):
    consolidation_groups[cluster].append(unique_tran_cd[i])

# Calculate the average cosine similarity for each group
consolidation_similarities = {}
for cluster, tran_cds in consolidation_groups.items():
    if len(tran_cds) > 1:
        group_similarities = cosine_sim_matrix.loc[tran_cds, tran_cds].values
        avg_similarity = np.mean(group_similarities[np.triu_indices_from(group_similarities, k=1)])
        consolidation_similarities[cluster] = (tran_cds, avg_similarity)

# Output the recommended consolidation groups
for cluster, (tran_cds, avg_similarity) in consolidation_similarities.items():
    print(f"Group {cluster}: TRAN_CDs {tran_cds} with average cosine similarity {avg_similarity:.2f}")

